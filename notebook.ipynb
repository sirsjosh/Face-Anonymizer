{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404d01db",
   "metadata": {},
   "source": [
    "<h1> Face Anonymizer </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f4361",
   "metadata": {},
   "source": [
    "<h2> imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bdb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe==0.9.3.0\n",
    "# !pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dc1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd27fc",
   "metadata": {},
   "source": [
    "<h2> detect faces and blur </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1dcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img, face_detection):\n",
    "    # Get image dimensions\n",
    "    H, W, _ = img.shape\n",
    "    # Convert image to RGB format (required by MediaPipe)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Run face detection on the image\n",
    "    out = face_detection.process(img_rgb)\n",
    "    \n",
    "    # Check if faces are detected\n",
    "    if out.detections is not None:\n",
    "        # Loop through each detected face\n",
    "        for detection in out.detections:\n",
    "            location_data = detection.location_data\n",
    "            bbox = location_data.relative_bounding_box\n",
    "\n",
    "            # Extract bounding box coordinates and scale to image dimensions\n",
    "            x1, y1, w, h = bbox.xmin, bbox.ymin ,bbox.width, bbox.height\n",
    "            x1 = int(x1 * W)\n",
    "            y1 = int(y1 * H)\n",
    "            w = int(w * W)\n",
    "            h = int(h * H)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Apply blur to the region of the face\n",
    "            img[y1:y1 + h, x1:x1 + w, :] = cv2.blur(img[y1:y1 + h, x1:x1 + w, :], (30, 30))\n",
    "            \n",
    "            # Draw a green rectangle around the detected face\n",
    "            img = cv2.rectangle(img, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 10)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d484488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# Parse command line arguments\n",
    "args = argparse.ArgumentParser()\n",
    "args.add_argument('--mode', default='video')\n",
    "args.add_argument('--filePath', default='testVideo.mp4')\n",
    "args = args.parse_args(args=[])\n",
    "\n",
    "# Initialize MediaPipe face detection module\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    # Choose mode of operation\n",
    "    if args.mode in ['image']:\n",
    "        img = cv2.imread(args.filePath)\n",
    "        img = process_img(img, face_detection)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "    elif args.mode in ['video']:\n",
    "        cap = cv2.VideoCapture(args.filePath)\n",
    "        ret, frame = cap.read()\n",
    "        # Initialize video writer for output\n",
    "        output_video = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 25, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        while ret:\n",
    "            frame = process_img(frame, face_detection)\n",
    "            # Write processed frame to output video\n",
    "            output_video.write(frame)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "        cap.release()\n",
    "        output_video.release()\n",
    "        \n",
    "    elif args.mode in ['webcam']:\n",
    "        cap = cv2.VideoCapture(1)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        while ret:\n",
    "            frame = process_img(frame, face_detection)\n",
    "            # Display the processed frame\n",
    "            cv2.imshow('frame', frame)\n",
    "            # Wait for 25 milliseconds (adjust as needed)\n",
    "            cv2.waitKey(25)\n",
    "            ret, frame = cap.read()\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d83c33",
   "metadata": {},
   "source": [
    "<h2> blur </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfeab91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
